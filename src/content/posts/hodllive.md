---
title: Hodllive
---

<img src="/posts/hodllive.png" alt="Screenshot of Hodllive which overlays line charts of subscriber counts for multiple VTubers" />

# Introduction

[Hodllive](https://www.speculative.tech/hodllive/) is a
[self-updating static site](#the-self-updating-static-site-pattern) that visualizes subscriber and
view counts for VTubers' YouTube channels.

In early 2020, VTubers rapidly began to rise in popularity. In VTuber culture, hitting certain
milestone subscriber counts is cause for celebration. I found myself manually visiting many VTubers'
channels every week to check their current subscriber counts to see how close they were to the next
milestone and compare their growth rates. Eventually, I decided to create a web app to visualize the
data.

The first group of VTubers that I watched consistently was Hololive, and the subscriber charts have
a tendency to go up and to the right, hence "HODLLive". Later on, I
[added VTubers from Nijisanji](#web-scraping-to-add-nijisanji) as well.

# The self-updating static site pattern

There are probably other people out there using this pattern, but I've never seen it written about.
Hodllive is designed to be a _self-updating_ static website: the application itself consists of a
static HTML and JS file hosted on GitHub pages which dynamically loads a JSON dataset hosted within
the same repo. The JSON dataset is generated by a
[short python script](https://github.com/Speculative/hodllive/blob/master/hodllive.py) that queries
the YouTube API for channel statistics. Every day, a
[GitHub Action](https://github.com/Speculative/hodllive/blob/master/.github/workflows/update_data.yml)
runs which clones the repo, updates the JSON data using the YouTube API, and commits it back to the
repo. As a result, there is no "back-end" for Hodllive.

This is a very convenient pattern if you have a reasonably small data set. As long as the update
script continues working[^1], the site will reliably stay online and have fresh data. There's no
reason the updates have to be daily: a friend of mine uses this recipe for
[a website that updates hourly](https://github.com/Lego6245/hundoLeaderboard). At time of writing,
GitHub offers 2,000 minutes of GitHub Actions time per month for their free tier, which works out to
over an hour of Action time per day. This should be plenty for most small apps.

An explicit goal for this project was for it to be as close to an old-school "website" as possible.
Hodllive uses Vue served from a CDN, and all of the application logic is contained in a single JS
file, so there is no need for a build step[^2]. "Deploying" new versions of the app is as simple as
committing and pushing to GitHub, similar to how old "websites" were deployed by uploading some
files to a server via FTP.

[^1]:
    In practice, I've had the update script break a few times for mysterious reasons that I don't
    fully understand. The most frequent cause seems to be failing to install dependencies for the
    update script.

[^2]: [Holocraft](/posts/holocraft) uses a variant of this pattern with a svelte build step.

# Web scraping to add Nijisanji

In July of 2022, I became interested in expanding Hodllive to include Nijisanji members. This was a
large project: Hololive had few enough members that I was able to scrape names, YouTube channel
URLs, generation membership, and colors manually. Nijisanji has nearly 200 members across all of its
branches (excluding VirtuaReal, whom I excluded because they stream on Bilibili, which I don't have
a subscriber retrieval script for). Furthermore, I wanted to backfill historical subscriber counts
to enable comparisons in the preceding 3+ years of history.

[Extracting VTuber metadata](https://github.com/Speculative/hodllive/blob/master/scripts/wiki_member_info.py)
was straightforward. The
[VTuber Wiki](<https://virtualyoutuber.fandom.com/wiki/NIJISANJI_(main_branch)#First_Generation>)
lists the members of each Nijisanji generation in a table, with the heading above the table
containing the name of the generation. Each entry in the table links to
[the member's individual wiki page](https://virtualyoutuber.fandom.com/wiki/Tsukino_Mito) which
includes their YouTube channel URL, debut date, full name, and official color[^3].

With each member's channel URL, I was able to retrieve historical revisions of their channel page
using [the Wayback Machine API](https://pypi.org/project/waybackpy/) and
[extract subscriber counts from the page content](https://github.com/Speculative/hodllive/blob/master/scripts/wayback_subs.py).
As I was scraping, I learned that there were a number of special cases to handle:

1. Sometimes, the scraped pages were in different languages. This required different parsing
   schemes, as YouTube embeds the subscriber count as localized text, and numeric order-of-magnitude
   suffixes vary from language to language. For example, Japanese-localized pages use ä¸‡, which is
   an increment of 10,000.
2. The structure of the page itself changed a few times over the years, requiring different schemes
   for finding the localized subscriber count text.
3. Sometimes, the Wayback Machine crawler was redirected to a GDPR cookie consent screen. In these
   cases, the subscriber count was lost.

This was an error-prone process, and I had to stop and restart the scraper multiple times. As there
were hundreds of channels, each with hundreds of historical pages to scrape, restarting the process
from scratch each time was untenable. Instead, I employed two techniques that I think made this
process much easier:

1. When the scraper encounteredd a page that it failed to scrape, instead of terminating with an
   exception, it called `pdb.set_trace()` so that I could interactively experiment with the page
   contents to add new scraping logic.
2. Scraped data points were stored as rows in a SQLite database. This made it easy to skip already
   scraped pages by checking to see if the page was already in the database, or force re-scraping of
   groups of pages by dropping the corresponding rows them from the database.

Finally, I encountered anomalies in the scraped data set where subscriber counts would jump wildly,
seemingly to values from the past or future, usually from a day or two before or after. I'm not sure
why this happens. It could be due to timezones, the Wayback Machine storing incorrect dates, or
perhaps consistency issues in YouTube itself. I observed these anomalies after retrieving and
visualizing all of the subscriber counts and fixed them by manually removing data points.

The quality of the end results varied from channel to channel. More popular VTubers tended to have
more complete Wayback Machine histories, perhaps due to someone manually requesting crawls, or due
to their channels being linked to more often from other sources. Now that all of the Nijisanji
members have been added to Hodllive, their channel statistics will be updated daily.

[^3]:
    As with any human-maintained data set, there were some irregularities. Some members don't have
    precise debut date or official colors, which I had to infer and fill in by hand.

# Limitations

The YouTube API provides 3 significant figures for subscriber counts. As such, when a channel
crosses the 100,000 and 1,000,000 subscriber milestones, precision decreases by a factor of 10. This
is most visually obvious for channels after crossing the 1,000,000 subscriber mark: most VTuber
channels tend not to grow by > 10,000 subscribers per day, so on the line plot, their subscriber
count is completely level for several days in a row before ticking up by 10,000. Unfortunately, this
is a fundamental limitation of the YouTube API, presumably in place so that YouTube can keep users
using its own analytics tools.

# Credits

Hodllive's cute favicon of Roboco-san's glasses was made by [PJ Rosa](https://pj.codes/).
