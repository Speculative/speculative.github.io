import{S as vs,i as ws,s as ys,k as r,a as u,q as s,l as o,c as f,m as n,r as i,h as a,O as gs,n as l,b as c,C as t,B as xa}from"./index-a9a908c3.js";function Is(ms){let w,Pa,He,T,at,N,st,je,y,Y,it,rt,ie,ot,nt,xe,re,lt,Pe,E,ht,oe,ct,ut,Le,k,ft,J,dt,Se,p,pt,ge,bt,mt,q,vt,wt,W,yt,gt,Ve,b,It,ne,M,_t,Tt,R,Et,kt,Ne,A,At,le,z,Ht,jt,Ye,H,xt,G,Pt,Je,he,Lt,qe,d,O,St,Vt,U,Nt,Yt,D,Jt,qt,ce,F,Wt,Mt,We,m,Rt,C,zt,Gt,B,Ot,Ut,Me,v,Ie,Dt,Ft,_e,Ct,Bt,Te,Qt,Re,ue,Kt,ze,j,Q,Xt,Ee,Zt,$t,ea,ke,ta,Ge,fe,aa,Oe,de,sa,Ue,x,ia,K,ra,De,pe,oa,Fe,P,na,X,la,Ce,L,ha,Z,ca,ua,Be,g,fa,da,I,$,pa,ee,ba,ma,S,be,va,wa,te,ya,ga,ae,Ia,se,_a;return{c(){w=r("img"),He=u(),T=r("h1"),at=s("Introduction"),N=r("a"),st=s("#"),je=u(),y=r("p"),Y=r("a"),it=s("Hodllive"),rt=s(` is a
`),ie=r("a"),ot=s("self-updating static site"),nt=s(` that visualizes subscriber and
view counts for VTubers\u2019 YouTube channels.`),xe=u(),re=r("p"),lt=s(`In early 2020, VTubers rapidly began to rise in popularity. In VTuber culture, hitting certain
milestone subscriber counts is cause for celebration. I found myself manually visiting many VTubers\u2019
channels every week to check their current subscriber counts to see how close they were to the next
milestone and compare their growth rates. Eventually, I decided to create a web app to visualize the
data.`),Pe=u(),E=r("p"),ht=s(`The first group of VTubers that I watched consistently was Hololive, and the subscriber charts have
a tendency to go up and to the right, hence \u201CHODLLive\u201D. Later on, I
`),oe=r("a"),ct=s("added VTubers from Nijisanji"),ut=s(" as well."),Le=u(),k=r("h1"),ft=s("The self-updating static site pattern"),J=r("a"),dt=s("#"),Se=u(),p=r("p"),pt=s(`There are probably other people out there using this pattern, but I\u2019ve never seen it written about.
Hodllive is designed to be a `),ge=r("em"),bt=s("self-updating"),mt=s(` static website: the application itself consists of a
static HTML and JS file hosted on GitHub pages which dynamically loads a JSON dataset hosted within
the same repo. The JSON dataset is generated by a
`),q=r("a"),vt=s("short python script"),wt=s(` that queries
the YouTube API for channel statistics. Every day, a
`),W=r("a"),yt=s("GitHub Action"),gt=s(`
runs which clones the repo, updates the JSON data using the YouTube API, and commits it back to the
repo. As a result, there is no \u201Cback-end\u201D for Hodllive.`),Ve=u(),b=r("p"),It=s(`This is a very convenient pattern if you have a reasonably small data set. As long as the update
script continues working`),ne=r("sup"),M=r("a"),_t=s("1"),Tt=s(`, the site will reliably stay online and have fresh data. There\u2019s no
reason the updates have to be daily: a friend of mine uses this recipe for
`),R=r("a"),Et=s("a website that updates hourly"),kt=s(`. At time of writing,
GitHub offers 2,000 minutes of GitHub Actions time per month for their free tier, which works out to
over an hour of Action time per day. This should be plenty for most small apps.`),Ne=u(),A=r("p"),At=s(`An explicit goal for this project was for it to be as close to an old-school \u201Cwebsite\u201D as possible.
Hodllive uses Vue served from a CDN, and all of the application logic is contained in a single JS
file, so there is no need for a build step`),le=r("sup"),z=r("a"),Ht=s("2"),jt=s(`. \u201CDeploying\u201D new versions of the app is as simple as
committing and pushing to GitHub, similar to how old \u201Cwebsites\u201D were deployed by uploading some
files to a server via FTP.`),Ye=u(),H=r("h1"),xt=s("Web scraping to add Nijisanji"),G=r("a"),Pt=s("#"),Je=u(),he=r("p"),Lt=s(`In July of 2022, I became interested in expanding Hodllive to include Nijisanji members. This was a
large project: Hololive had few enough members that I was able to scrape names, YouTube channel
URLs, generation membership, and colors manually. Nijisanji has nearly 200 members across all of its
branches (excluding VirtuaReal, whom I excluded because they stream on Bilibili, which I don\u2019t have
a subscriber retrieval script for). Furthermore, I wanted to backfill historical subscriber counts
to enable comparisons in the preceding 3+ years of history.`),qe=u(),d=r("p"),O=r("a"),St=s("Extracting VTuber metadata"),Vt=s(`
was straightforward. The
`),U=r("a"),Nt=s("VTuber Wiki"),Yt=s(`
lists the members of each Nijisanji generation in a table, with the heading above the table
containing the name of the generation. Each entry in the table links to
`),D=r("a"),Jt=s("the member\u2019s individual wiki page"),qt=s(` which
includes their YouTube channel URL, debut date, full name, and official color`),ce=r("sup"),F=r("a"),Wt=s("3"),Mt=s("."),We=u(),m=r("p"),Rt=s(`With each member\u2019s channel URL, I was able to retrieve historical revisions of their channel page
using `),C=r("a"),zt=s("the Wayback Machine API"),Gt=s(` and
`),B=r("a"),Ot=s("extract subscriber counts from the page content"),Ut=s(`.
As I was scraping, I learned that there were a number of special cases to handle:`),Me=u(),v=r("ol"),Ie=r("li"),Dt=s(`Sometimes, the scraped pages were in different languages. This required different parsing
schemes, as YouTube embeds the subscriber count as localized text, and numeric order-of-magnitude
suffixes vary from language to language. For example, Japanese-localized pages use \u4E07, which is
an increment of 10,000.`),Ft=u(),_e=r("li"),Ct=s(`The structure of the page itself changed a few times over the years, requiring different schemes
for finding the localized subscriber count text.`),Bt=u(),Te=r("li"),Qt=s(`Sometimes, the Wayback Machine crawler was redirected to a GDPR cookie consent screen. In these
cases, the subscriber count was lost.`),Re=u(),ue=r("p"),Kt=s(`This was an error-prone process, and I had to stop and restart the scraper multiple times. As there
were hundreds of channels, each with hundreds of historical pages to scrape, restarting the process
from scratch each time was untenable. Instead, I employed two techniques that I think made this
process much easier:`),ze=u(),j=r("ol"),Q=r("li"),Xt=s(`When the scraper encounteredd a page that it failed to scrape, instead of terminating with an
exception, it called `),Ee=r("code"),Zt=s("pdb.set_trace()"),$t=s(` so that I could interactively experiment with the page
contents to add new scraping logic.`),ea=u(),ke=r("li"),ta=s(`Scraped data points were stored as rows in a SQLite database. This made it easy to skip already
scraped pages by checking to see if the page was already in the database, or force re-scraping of
groups of pages by dropping the corresponding rows them from the database.`),Ge=u(),fe=r("p"),aa=s(`Finally, I encountered anomalies in the scraped data set where subscriber counts would jump wildly,
seemingly to values from the past or future, usually from a day or two before or after. I\u2019m not sure
why this happens. It could be due to timezones, the Wayback Machine storing incorrect dates, or
perhaps consistency issues in YouTube itself. I observed these anomalies after retrieving and
visualizing all of the subscriber counts and fixed them by manually removing data points.`),Oe=u(),de=r("p"),sa=s(`The quality of the end results varied from channel to channel. More popular VTubers tended to have
more complete Wayback Machine histories, perhaps due to someone manually requesting crawls, or due
to their channels being linked to more often from other sources. Now that all of the Nijisanji
members have been added to Hodllive, their channel statistics will be updated daily.`),Ue=u(),x=r("h1"),ia=s("Limitations"),K=r("a"),ra=s("#"),De=u(),pe=r("p"),oa=s(`The YouTube API provides 3 significant figures for subscriber counts. As such, when a channel
crosses the 100,000 and 1,000,000 subscriber milestones, precision decreases by a factor of 10. This
is most visually obvious for channels after crossing the 1,000,000 subscriber mark: most VTuber
channels tend not to grow by > 10,000 subscribers per day, so on the line plot, their subscriber
count is completely level for several days in a row before ticking up by 10,000. Unfortunately, this
is a fundamental limitation of the YouTube API, presumably in place so that YouTube can keep users
using its own analytics tools.`),Fe=u(),P=r("h1"),na=s("Credits"),X=r("a"),la=s("#"),Ce=u(),L=r("p"),ha=s("Hodllive\u2019s cute favicon of Roboco-san\u2019s glasses was made by "),Z=r("a"),ca=s("PJ Rosa"),ua=s("."),Be=u(),g=r("div"),fa=r("hr"),da=u(),I=r("ol"),$=r("li"),pa=s(`In practice, I\u2019ve had the update script break a few times for mysterious reasons that I don\u2019t
fully understand. The most frequent cause seems to be failing to install dependencies for the
update script.`),ee=r("a"),ba=s("\u21A9"),ma=u(),S=r("li"),be=r("a"),va=s("Holocraft"),wa=s(" uses a variant of this pattern with a svelte build step."),te=r("a"),ya=s("\u21A9"),ga=u(),ae=r("li"),Ia=s(`As with any human-maintained data set, there were some irregularities. Some members don\u2019t have
precise debut date or official colors, which I had to infer and fill in by hand.`),se=r("a"),_a=s("\u21A9"),this.h()},l(e){w=o(e,"IMG",{src:!0,alt:!0}),He=f(e),T=o(e,"H1",{id:!0});var h=n(T);at=i(h,"Introduction"),N=o(h,"A",{class:!0,href:!0});var La=n(N);st=i(La,"#"),La.forEach(a),h.forEach(a),je=f(e),y=o(e,"P",{});var Ae=n(y);Y=o(Ae,"A",{href:!0,rel:!0});var Sa=n(Y);it=i(Sa,"Hodllive"),Sa.forEach(a),rt=i(Ae,` is a
`),ie=o(Ae,"A",{href:!0});var Va=n(ie);ot=i(Va,"self-updating static site"),Va.forEach(a),nt=i(Ae,` that visualizes subscriber and
view counts for VTubers\u2019 YouTube channels.`),Ae.forEach(a),xe=f(e),re=o(e,"P",{});var Na=n(re);lt=i(Na,`In early 2020, VTubers rapidly began to rise in popularity. In VTuber culture, hitting certain
milestone subscriber counts is cause for celebration. I found myself manually visiting many VTubers\u2019
channels every week to check their current subscriber counts to see how close they were to the next
milestone and compare their growth rates. Eventually, I decided to create a web app to visualize the
data.`),Na.forEach(a),Pe=f(e),E=o(e,"P",{});var Qe=n(E);ht=i(Qe,`The first group of VTubers that I watched consistently was Hololive, and the subscriber charts have
a tendency to go up and to the right, hence \u201CHODLLive\u201D. Later on, I
`),oe=o(Qe,"A",{href:!0});var Ya=n(oe);ct=i(Ya,"added VTubers from Nijisanji"),Ya.forEach(a),ut=i(Qe," as well."),Qe.forEach(a),Le=f(e),k=o(e,"H1",{id:!0});var Ta=n(k);ft=i(Ta,"The self-updating static site pattern"),J=o(Ta,"A",{class:!0,href:!0});var Ja=n(J);dt=i(Ja,"#"),Ja.forEach(a),Ta.forEach(a),Se=f(e),p=o(e,"P",{});var V=n(p);pt=i(V,`There are probably other people out there using this pattern, but I\u2019ve never seen it written about.
Hodllive is designed to be a `),ge=o(V,"EM",{});var qa=n(ge);bt=i(qa,"self-updating"),qa.forEach(a),mt=i(V,` static website: the application itself consists of a
static HTML and JS file hosted on GitHub pages which dynamically loads a JSON dataset hosted within
the same repo. The JSON dataset is generated by a
`),q=o(V,"A",{href:!0,rel:!0});var Wa=n(q);vt=i(Wa,"short python script"),Wa.forEach(a),wt=i(V,` that queries
the YouTube API for channel statistics. Every day, a
`),W=o(V,"A",{href:!0,rel:!0});var Ma=n(W);yt=i(Ma,"GitHub Action"),Ma.forEach(a),gt=i(V,`
runs which clones the repo, updates the JSON data using the YouTube API, and commits it back to the
repo. As a result, there is no \u201Cback-end\u201D for Hodllive.`),V.forEach(a),Ve=f(e),b=o(e,"P",{});var me=n(b);It=i(me,`This is a very convenient pattern if you have a reasonably small data set. As long as the update
script continues working`),ne=o(me,"SUP",{id:!0});var Ra=n(ne);M=o(Ra,"A",{href:!0,class:!0});var za=n(M);_t=i(za,"1"),za.forEach(a),Ra.forEach(a),Tt=i(me,`, the site will reliably stay online and have fresh data. There\u2019s no
reason the updates have to be daily: a friend of mine uses this recipe for
`),R=o(me,"A",{href:!0,rel:!0});var Ga=n(R);Et=i(Ga,"a website that updates hourly"),Ga.forEach(a),kt=i(me,`. At time of writing,
GitHub offers 2,000 minutes of GitHub Actions time per month for their free tier, which works out to
over an hour of Action time per day. This should be plenty for most small apps.`),me.forEach(a),Ne=f(e),A=o(e,"P",{});var Ke=n(A);At=i(Ke,`An explicit goal for this project was for it to be as close to an old-school \u201Cwebsite\u201D as possible.
Hodllive uses Vue served from a CDN, and all of the application logic is contained in a single JS
file, so there is no need for a build step`),le=o(Ke,"SUP",{id:!0});var Oa=n(le);z=o(Oa,"A",{href:!0,class:!0});var Ua=n(z);Ht=i(Ua,"2"),Ua.forEach(a),Oa.forEach(a),jt=i(Ke,`. \u201CDeploying\u201D new versions of the app is as simple as
committing and pushing to GitHub, similar to how old \u201Cwebsites\u201D were deployed by uploading some
files to a server via FTP.`),Ke.forEach(a),Ye=f(e),H=o(e,"H1",{id:!0});var Ea=n(H);xt=i(Ea,"Web scraping to add Nijisanji"),G=o(Ea,"A",{class:!0,href:!0});var Da=n(G);Pt=i(Da,"#"),Da.forEach(a),Ea.forEach(a),Je=f(e),he=o(e,"P",{});var Fa=n(he);Lt=i(Fa,`In July of 2022, I became interested in expanding Hodllive to include Nijisanji members. This was a
large project: Hololive had few enough members that I was able to scrape names, YouTube channel
URLs, generation membership, and colors manually. Nijisanji has nearly 200 members across all of its
branches (excluding VirtuaReal, whom I excluded because they stream on Bilibili, which I don\u2019t have
a subscriber retrieval script for). Furthermore, I wanted to backfill historical subscriber counts
to enable comparisons in the preceding 3+ years of history.`),Fa.forEach(a),qe=f(e),d=o(e,"P",{});var _=n(d);O=o(_,"A",{href:!0,rel:!0});var Ca=n(O);St=i(Ca,"Extracting VTuber metadata"),Ca.forEach(a),Vt=i(_,`
was straightforward. The
`),U=o(_,"A",{href:!0,rel:!0});var Ba=n(U);Nt=i(Ba,"VTuber Wiki"),Ba.forEach(a),Yt=i(_,`
lists the members of each Nijisanji generation in a table, with the heading above the table
containing the name of the generation. Each entry in the table links to
`),D=o(_,"A",{href:!0,rel:!0});var Qa=n(D);Jt=i(Qa,"the member\u2019s individual wiki page"),Qa.forEach(a),qt=i(_,` which
includes their YouTube channel URL, debut date, full name, and official color`),ce=o(_,"SUP",{id:!0});var Ka=n(ce);F=o(Ka,"A",{href:!0,class:!0});var Xa=n(F);Wt=i(Xa,"3"),Xa.forEach(a),Ka.forEach(a),Mt=i(_,"."),_.forEach(a),We=f(e),m=o(e,"P",{});var ve=n(m);Rt=i(ve,`With each member\u2019s channel URL, I was able to retrieve historical revisions of their channel page
using `),C=o(ve,"A",{href:!0,rel:!0});var Za=n(C);zt=i(Za,"the Wayback Machine API"),Za.forEach(a),Gt=i(ve,` and
`),B=o(ve,"A",{href:!0,rel:!0});var $a=n(B);Ot=i($a,"extract subscriber counts from the page content"),$a.forEach(a),Ut=i(ve,`.
As I was scraping, I learned that there were a number of special cases to handle:`),ve.forEach(a),Me=f(e),v=o(e,"OL",{});var we=n(v);Ie=o(we,"LI",{});var es=n(Ie);Dt=i(es,`Sometimes, the scraped pages were in different languages. This required different parsing
schemes, as YouTube embeds the subscriber count as localized text, and numeric order-of-magnitude
suffixes vary from language to language. For example, Japanese-localized pages use \u4E07, which is
an increment of 10,000.`),es.forEach(a),Ft=f(we),_e=o(we,"LI",{});var ts=n(_e);Ct=i(ts,`The structure of the page itself changed a few times over the years, requiring different schemes
for finding the localized subscriber count text.`),ts.forEach(a),Bt=f(we),Te=o(we,"LI",{});var as=n(Te);Qt=i(as,`Sometimes, the Wayback Machine crawler was redirected to a GDPR cookie consent screen. In these
cases, the subscriber count was lost.`),as.forEach(a),we.forEach(a),Re=f(e),ue=o(e,"P",{});var ss=n(ue);Kt=i(ss,`This was an error-prone process, and I had to stop and restart the scraper multiple times. As there
were hundreds of channels, each with hundreds of historical pages to scrape, restarting the process
from scratch each time was untenable. Instead, I employed two techniques that I think made this
process much easier:`),ss.forEach(a),ze=f(e),j=o(e,"OL",{});var Xe=n(j);Q=o(Xe,"LI",{});var Ze=n(Q);Xt=i(Ze,`When the scraper encounteredd a page that it failed to scrape, instead of terminating with an
exception, it called `),Ee=o(Ze,"CODE",{});var is=n(Ee);Zt=i(is,"pdb.set_trace()"),is.forEach(a),$t=i(Ze,` so that I could interactively experiment with the page
contents to add new scraping logic.`),Ze.forEach(a),ea=f(Xe),ke=o(Xe,"LI",{});var rs=n(ke);ta=i(rs,`Scraped data points were stored as rows in a SQLite database. This made it easy to skip already
scraped pages by checking to see if the page was already in the database, or force re-scraping of
groups of pages by dropping the corresponding rows them from the database.`),rs.forEach(a),Xe.forEach(a),Ge=f(e),fe=o(e,"P",{});var os=n(fe);aa=i(os,`Finally, I encountered anomalies in the scraped data set where subscriber counts would jump wildly,
seemingly to values from the past or future, usually from a day or two before or after. I\u2019m not sure
why this happens. It could be due to timezones, the Wayback Machine storing incorrect dates, or
perhaps consistency issues in YouTube itself. I observed these anomalies after retrieving and
visualizing all of the subscriber counts and fixed them by manually removing data points.`),os.forEach(a),Oe=f(e),de=o(e,"P",{});var ns=n(de);sa=i(ns,`The quality of the end results varied from channel to channel. More popular VTubers tended to have
more complete Wayback Machine histories, perhaps due to someone manually requesting crawls, or due
to their channels being linked to more often from other sources. Now that all of the Nijisanji
members have been added to Hodllive, their channel statistics will be updated daily.`),ns.forEach(a),Ue=f(e),x=o(e,"H1",{id:!0});var ka=n(x);ia=i(ka,"Limitations"),K=o(ka,"A",{class:!0,href:!0});var ls=n(K);ra=i(ls,"#"),ls.forEach(a),ka.forEach(a),De=f(e),pe=o(e,"P",{});var hs=n(pe);oa=i(hs,`The YouTube API provides 3 significant figures for subscriber counts. As such, when a channel
crosses the 100,000 and 1,000,000 subscriber milestones, precision decreases by a factor of 10. This
is most visually obvious for channels after crossing the 1,000,000 subscriber mark: most VTuber
channels tend not to grow by > 10,000 subscribers per day, so on the line plot, their subscriber
count is completely level for several days in a row before ticking up by 10,000. Unfortunately, this
is a fundamental limitation of the YouTube API, presumably in place so that YouTube can keep users
using its own analytics tools.`),hs.forEach(a),Fe=f(e),P=o(e,"H1",{id:!0});var Aa=n(P);na=i(Aa,"Credits"),X=o(Aa,"A",{class:!0,href:!0});var cs=n(X);la=i(cs,"#"),cs.forEach(a),Aa.forEach(a),Ce=f(e),L=o(e,"P",{});var $e=n(L);ha=i($e,"Hodllive\u2019s cute favicon of Roboco-san\u2019s glasses was made by "),Z=o($e,"A",{href:!0,rel:!0});var us=n(Z);ca=i(us,"PJ Rosa"),us.forEach(a),ua=i($e,"."),$e.forEach(a),Be=f(e),g=o(e,"DIV",{class:!0});var et=n(g);fa=o(et,"HR",{}),da=f(et),I=o(et,"OL",{});var ye=n(I);$=o(ye,"LI",{id:!0});var Ha=n($);pa=i(Ha,`In practice, I\u2019ve had the update script break a few times for mysterious reasons that I don\u2019t
fully understand. The most frequent cause seems to be failing to install dependencies for the
update script.`),ee=o(Ha,"A",{href:!0,class:!0});var fs=n(ee);ba=i(fs,"\u21A9"),fs.forEach(a),Ha.forEach(a),ma=f(ye),S=o(ye,"LI",{id:!0});var tt=n(S);be=o(tt,"A",{href:!0});var ds=n(be);va=i(ds,"Holocraft"),ds.forEach(a),wa=i(tt," uses a variant of this pattern with a svelte build step."),te=o(tt,"A",{href:!0,class:!0});var ps=n(te);ya=i(ps,"\u21A9"),ps.forEach(a),tt.forEach(a),ga=f(ye),ae=o(ye,"LI",{id:!0});var ja=n(ae);Ia=i(ja,`As with any human-maintained data set, there were some irregularities. Some members don\u2019t have
precise debut date or official colors, which I had to infer and fill in by hand.`),se=o(ja,"A",{href:!0,class:!0});var bs=n(se);_a=i(bs,"\u21A9"),bs.forEach(a),ja.forEach(a),ye.forEach(a),et.forEach(a),this.h()},h(){gs(w.src,Pa="/posts/hodllive.png")||l(w,"src",Pa),l(w,"alt","Screenshot of Hodllive which overlays line charts of subscriber counts for multiple VTubers"),l(N,"class","heading-anchor"),l(N,"href","#introduction"),l(T,"id","introduction"),l(Y,"href","https://www.speculative.tech/hodllive/"),l(Y,"rel","nofollow"),l(ie,"href","#the-self-updating-static-site-pattern"),l(oe,"href","#web-scraping-to-add-nijisanji"),l(J,"class","heading-anchor"),l(J,"href","#the-self-updating-static-site-pattern"),l(k,"id","the-self-updating-static-site-pattern"),l(q,"href","https://github.com/Speculative/hodllive/blob/master/hodllive.py"),l(q,"rel","nofollow"),l(W,"href","https://github.com/Speculative/hodllive/blob/master/.github/workflows/update_data.yml"),l(W,"rel","nofollow"),l(M,"href","#fn-1"),l(M,"class","footnote-ref"),l(ne,"id","fnref-1"),l(R,"href","https://github.com/Lego6245/hundoLeaderboard"),l(R,"rel","nofollow"),l(z,"href","#fn-2"),l(z,"class","footnote-ref"),l(le,"id","fnref-2"),l(G,"class","heading-anchor"),l(G,"href","#web-scraping-to-add-nijisanji"),l(H,"id","web-scraping-to-add-nijisanji"),l(O,"href","https://github.com/Speculative/hodllive/blob/master/scripts/wiki_member_info.py"),l(O,"rel","nofollow"),l(U,"href","https://virtualyoutuber.fandom.com/wiki/NIJISANJI_(main_branch)#First_Generation"),l(U,"rel","nofollow"),l(D,"href","https://virtualyoutuber.fandom.com/wiki/Tsukino_Mito"),l(D,"rel","nofollow"),l(F,"href","#fn-3"),l(F,"class","footnote-ref"),l(ce,"id","fnref-3"),l(C,"href","https://pypi.org/project/waybackpy/"),l(C,"rel","nofollow"),l(B,"href","https://github.com/Speculative/hodllive/blob/master/scripts/wayback_subs.py"),l(B,"rel","nofollow"),l(K,"class","heading-anchor"),l(K,"href","#limitations"),l(x,"id","limitations"),l(X,"class","heading-anchor"),l(X,"href","#credits"),l(P,"id","credits"),l(Z,"href","https://pj.codes/"),l(Z,"rel","nofollow"),l(ee,"href","#fnref-1"),l(ee,"class","footnote-backref"),l($,"id","fn-1"),l(be,"href","/posts/holocraft"),l(te,"href","#fnref-2"),l(te,"class","footnote-backref"),l(S,"id","fn-2"),l(se,"href","#fnref-3"),l(se,"class","footnote-backref"),l(ae,"id","fn-3"),l(g,"class","footnotes")},m(e,h){c(e,w,h),c(e,He,h),c(e,T,h),t(T,at),t(T,N),t(N,st),c(e,je,h),c(e,y,h),t(y,Y),t(Y,it),t(y,rt),t(y,ie),t(ie,ot),t(y,nt),c(e,xe,h),c(e,re,h),t(re,lt),c(e,Pe,h),c(e,E,h),t(E,ht),t(E,oe),t(oe,ct),t(E,ut),c(e,Le,h),c(e,k,h),t(k,ft),t(k,J),t(J,dt),c(e,Se,h),c(e,p,h),t(p,pt),t(p,ge),t(ge,bt),t(p,mt),t(p,q),t(q,vt),t(p,wt),t(p,W),t(W,yt),t(p,gt),c(e,Ve,h),c(e,b,h),t(b,It),t(b,ne),t(ne,M),t(M,_t),t(b,Tt),t(b,R),t(R,Et),t(b,kt),c(e,Ne,h),c(e,A,h),t(A,At),t(A,le),t(le,z),t(z,Ht),t(A,jt),c(e,Ye,h),c(e,H,h),t(H,xt),t(H,G),t(G,Pt),c(e,Je,h),c(e,he,h),t(he,Lt),c(e,qe,h),c(e,d,h),t(d,O),t(O,St),t(d,Vt),t(d,U),t(U,Nt),t(d,Yt),t(d,D),t(D,Jt),t(d,qt),t(d,ce),t(ce,F),t(F,Wt),t(d,Mt),c(e,We,h),c(e,m,h),t(m,Rt),t(m,C),t(C,zt),t(m,Gt),t(m,B),t(B,Ot),t(m,Ut),c(e,Me,h),c(e,v,h),t(v,Ie),t(Ie,Dt),t(v,Ft),t(v,_e),t(_e,Ct),t(v,Bt),t(v,Te),t(Te,Qt),c(e,Re,h),c(e,ue,h),t(ue,Kt),c(e,ze,h),c(e,j,h),t(j,Q),t(Q,Xt),t(Q,Ee),t(Ee,Zt),t(Q,$t),t(j,ea),t(j,ke),t(ke,ta),c(e,Ge,h),c(e,fe,h),t(fe,aa),c(e,Oe,h),c(e,de,h),t(de,sa),c(e,Ue,h),c(e,x,h),t(x,ia),t(x,K),t(K,ra),c(e,De,h),c(e,pe,h),t(pe,oa),c(e,Fe,h),c(e,P,h),t(P,na),t(P,X),t(X,la),c(e,Ce,h),c(e,L,h),t(L,ha),t(L,Z),t(Z,ca),t(L,ua),c(e,Be,h),c(e,g,h),t(g,fa),t(g,da),t(g,I),t(I,$),t($,pa),t($,ee),t(ee,ba),t(I,ma),t(I,S),t(S,be),t(be,va),t(S,wa),t(S,te),t(te,ya),t(I,ga),t(I,ae),t(ae,Ia),t(ae,se),t(se,_a)},p:xa,i:xa,o:xa,d(e){e&&a(w),e&&a(He),e&&a(T),e&&a(je),e&&a(y),e&&a(xe),e&&a(re),e&&a(Pe),e&&a(E),e&&a(Le),e&&a(k),e&&a(Se),e&&a(p),e&&a(Ve),e&&a(b),e&&a(Ne),e&&a(A),e&&a(Ye),e&&a(H),e&&a(Je),e&&a(he),e&&a(qe),e&&a(d),e&&a(We),e&&a(m),e&&a(Me),e&&a(v),e&&a(Re),e&&a(ue),e&&a(ze),e&&a(j),e&&a(Ge),e&&a(fe),e&&a(Oe),e&&a(de),e&&a(Ue),e&&a(x),e&&a(De),e&&a(pe),e&&a(Fe),e&&a(P),e&&a(Ce),e&&a(L),e&&a(Be),e&&a(g)}}}const Ts={title:"Hodllive"};class Es extends vs{constructor(w){super(),ws(this,w,null,Is,ys,{})}}export{Es as default,Ts as metadata};
